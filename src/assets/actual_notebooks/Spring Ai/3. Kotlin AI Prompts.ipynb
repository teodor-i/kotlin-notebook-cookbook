{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prompts\n",
    "\n",
    "Prompts are essentially the instructions or questions you give to AI models to get them to generate specific responses.\\\n",
    "Think of them as the conversation starter that guides what the AI will say back to you.\n",
    "\n",
    "This has even given rise to a field called \"Prompt Engineering\" — a discipline focused on developing and optimizing prompts for effective use of language models.\n",
    "\n",
    "In this notebook, we'll explore prompts in Spring AI:\n",
    "* Basic prompts\n",
    "* Message types\n",
    "* Prompting techniques\n",
    "\n",
    "Let's add the necessary dependencies:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:40.934482Z",
     "start_time": "2025-12-01T13:54:40.550089Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-anthropic"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use the model, we need to provide an API key.\n",
    "\n",
    "You can obtain this API key from\n",
    "[console.anthropic.com](https://console.anthropic.com/settings/keys)\n",
    "for Anthropic models or from\n",
    "[platform.openai.com](https://platform.openai.com/api-keys)\n",
    "for OpenAI models.\n",
    "\n",
    "Then add the generated API key to your environment variables:\n",
    "\n",
    "[MacOS/Linux]\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "export OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "\n",
    "```\n",
    "\n",
    "[Windows]\n",
    "```shell\n",
    "set ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "set OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "```\n",
    "\n",
    "Let's retrieve the API key from environment variables:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:41.034134Z",
     "start_time": "2025-12-01T13:54:40.935104Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "val apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"YOUR_ANTHROPIC_API_KEY\"",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like in the **`Intro`** notebook, let's create `ChatOptions` and a `ChatModel`."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:41.397726Z",
     "start_time": "2025-12-01T13:54:41.036771Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicApi = AnthropicApi.builder().apiKey(apiKey).build()\n",
    "val anthropicOptions = AnthropicChatOptions.builder()\n",
    "    .model(AnthropicApi.ChatModel.CLAUDE_SONNET_4_0)\n",
    "    .temperature(0.7)\n",
    "    .maxTokens(1024)\n",
    "    .build()\n",
    "\n",
    "val chatCompletion = AnthropicChatModel.builder()\n",
    "    .anthropicApi(anthropicApi)\n",
    "    .defaultOptions(anthropicOptions)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What makes a prompt?\n",
    "\n",
    "A prompt is simply a text request: \"tell me a joke\" or \"write a poem about mountains\"\n",
    "\n",
    "Let's ask our LLM to generate a haiku:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:44.500664Z",
     "start_time": "2025-12-01T13:54:41.398582Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatCompletion.call(\"Generate a hokku\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Here's a hokku for you:\n",
       "\n",
       "Morning frost melts—\n",
       "a single drop catches light\n",
       "on the bamboo leaf"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we're using `ChatClient` and the `Prompt` class, the request would look like this:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:47.433035Z",
     "start_time": "2025-12-01T13:54:44.501300Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatClient = ChatClient.create(chatCompletion)\n",
    "\n",
    "val prompt = Prompt(\"Generate a hokku\")\n",
    "chatClient.prompt(prompt).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Here's a hokku for you:\n",
       "\n",
       "Morning dew clings tight—\n",
       "even the spider's silk holds\n",
       "autumn's first farewell"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Type of messages\n",
    "\n",
    "In AI interactions, there are several message types (roles):\n",
    "* User — message from the user\n",
    "* Assistant — message from the AI\n",
    "* System — instructions that guide the AI's behavior\n",
    "* Tool — used for function calling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### User messages\n",
    "\n",
    "We've been writing user messages all along.\n",
    "\n",
    "Let's explicitly define them now:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:53.035472Z",
     "start_time": "2025-12-01T13:54:47.433836Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = Prompt(UserMessage(\"Generate a hokku\"), UserMessage(\"what's name of this hokku?\"))\n",
    "chatClient.prompt(messages).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Here's a hokku I've created:\n",
       "\n",
       "**Morning dew glistens**\n",
       "**on the spider's silver web—**\n",
       "**autumn light breaking**\n",
       "\n",
       "Regarding the name: Traditional hokku (and haiku) typically don't have titles. They are meant to stand alone as complete, self-contained moments of observation. The poem itself is simply called a \"hokku\" - that's both the form and the identifier. If you needed to reference this particular one, you might refer to it by its first line: \"Morning dew glistens.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### System messages\n",
    "\n",
    "A system message tells the LLM how it should behave.\n",
    "In Spring-AI, you can send a system message in several ways.\n",
    "\n",
    "For instance, you can use a special function for `ChatClient` or create an instance of a system message and pass it to the LLM directly."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-01T13:54:53.036058Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_9_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt()\n",
    "    .system(\"You are a financial expert. Answer briefly.\")\n",
    "    .user(\"If I had a time machine, should I buy Bitcoin in 2011?\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes, absolutely. Bitcoin was trading for around $1-$30 in 2011 and peaked near $69,000 in 2021 - that's roughly a 2,000-69,000x return depending on when in 2011 you bought.\n",
       "\n",
       "However, keep in mind:\n",
       "- You'd need diamond hands to hold through multiple 80%+ crashes\n",
       "- Early exchanges were risky (Mt. Gox collapsed in 2014)\n",
       "- You'd need secure storage methods that didn't exist yet\n",
       "- The regulatory landscape was completely unknown\n",
       "\n",
       "But yes, even buying a small amount and holding would have been life-changing wealth creation."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val messages = listOf(\n",
    "    SystemMessage(\"You are a financial expert. Answer briefly.\"),\n",
    "    UserMessage(\"If I had a time machine, should I buy Bitcoin in 2011?\")\n",
    ")\n",
    "\n",
    "chatClient.prompt(Prompt(messages)).call().content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Assistant messages\n",
    "\n",
    "As mentioned earlier, an assistant message is essentially a message from the LLM.\n",
    "\n",
    "Let's create a simple conversation example:\n",
    "we'll give the LLM a system instruction,\n",
    "send a request,\n",
    "and after receiving a response,\n",
    "ask for clarification on a specific point."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val messages = mutableListOf(\n",
    "    SystemMessage(\"You are an assistant who always answers very briefly, using no more than 10 words.\"),\n",
    "    UserMessage(\"Tell me about the capital of France and its landmarks\")\n",
    ")\n",
    "val assistantMessage = chatClient.prompt(Prompt(messages.toList())).call().chatResponse()!!.result.output\n",
    "assistantMessage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages.add(assistantMessage)\n",
    "messages.add(UserMessage(\"Tell me about the third landmark\"))\n",
    "\n",
    "chatClient.prompt(Prompt(messages.toList())).call().content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We've essentially created a simple dialogue between human and machine.\n",
    "\n",
    "Notice that in the last message,\n",
    "we deliberately formulated the request as:\n",
    "`\"Tell me about the third landmark\"`.\n",
    "If we hadn't sent this request along with the LLM's response about French landmarks,\n",
    "we wouldn't have received a meaningful answer."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt templates\n",
    "\n",
    "Spring AI provides PromptTemplate for working with prompts. This class uses the OSS\n",
    "[String Template](https://www.stringtemplate.org/)\n",
    "engine developed by Terence Parr for constructing and managing prompts.\n",
    "\n",
    "This class allows you to use resources for prompts, making them easier to manage and localize in different languages.\n",
    "It also lets you insert your data into the prompt,\n",
    "which is especially useful when developing RAG applications (which we'll learn more about in future notebooks).\n",
    "\n",
    "Let's write a simple example using PromptTemplate:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fun capital(): PromptTemplate {\n",
    "    val message = \"The two largest cities in {country}\"\n",
    "    return PromptTemplate(message)\n",
    "}\n",
    "\n",
    "val prompt = capital().create(mapOf(\"country\" to \"France\"))\n",
    "chatClient.prompt(prompt).call().content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompting techniques\n",
    "\n",
    "There are various techniques for crafting effective prompts that help us get better results from language models.\n",
    "These techniques range from simple to complex and can dramatically improve the quality of AI responses.\n",
    "\n",
    "We will consider techniques such as:\n",
    "* Zero-Shot Prompting\n",
    "* Few-Shot Prompting\n",
    "* Chain-of-Thought (CoT) Prompting\n",
    "* Meta Prompting\n",
    "* Generate Knowledge Prompting\n",
    "* Prompt Chaining\n",
    "\n",
    "These are far from all the techniques. There are many more."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting is a technique where the model performs a task based on direct instruction without being provided examples or demonstrations.\n",
    "The model relies solely on its pre-training knowledge."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Classify the text as neutral, negative, or positive.\n",
    "    Text: In my opinion, this restaurant is quite ordinary.\n",
    "    Tonality:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting is a technique where several examples (demonstrations) of task performance are included in the prompt to help the model understand exactly how to perform a similar task.\n",
    "Instead of training the model from scratch, we provide context through examples directly in the prompt."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    \"Zumbrik\" is a small fluffy animal inhabiting the Altai Mountains. Example sentence with the word zumbrik:\n",
    "    During our expedition to the Altai Mountains we met a family of cute zumbriks.\n",
    "\n",
    "    \"Fyrkotat\" means to quickly rotate in one place. Example sentence with the word fyrkotat:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, we provided the model with one example (1-shot) of using a made-up word in a sentence.\n",
    "Based on this,\n",
    "the model understood the task and was able to create a similar sentence with another made-up word,\n",
    "following the demonstrated pattern."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-Thought is a prompting technique that encourages the model to show intermediate steps of reasoning before providing the final answer.\n",
    "This is especially useful for complex tasks requiring mathematical calculations,\n",
    "logical analysis, or multi-step reasoning."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    In the group of numbers 15, 8, 3, 22, 7, 14, 26, do the odd numbers sum to an even number?\n",
    "    Let's reason step by step.\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Meta Prompting\n",
    "\n",
    "Meta prompting is an advanced technique that focuses on the structural and syntactic aspects of tasks rather than specific content details.\n",
    "It creates an abstract, structured way of interacting with the LLM,\n",
    "where the form and pattern of information are more important than the content itself."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    In problems of the format [problem P → solution S], follow this structure:\n",
    "    1. Define the variables from P\n",
    "    2. Construct an equation based on P\n",
    "    3. Solve the equation to find S\n",
    "    4. Verify the solution by substitution\n",
    "\n",
    "    Problem: A store had x apples. After selling 15 apples and then another 1/3 of the remaining apples, the store had 20 apples left. How many apples were there initially?\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, the meta prompt sets a general structure for approaching problem-solving,\n",
    "not focusing on specific content but offering a universal template for analysis and solution.\n",
    "The model follows this structure, applying it to the specific problem."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate Knowledge Prompting\n",
    "\n",
    "Generate Knowledge Prompting is a technique where the model first generates factual knowledge about a topic and then uses this knowledge to form a more accurate and well-founded answer.\n",
    "This technique is especially useful for tasks requiring common sense or factual accuracy."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val knowledge = chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Request: Are lichens harmful to trees?\n",
    "    Generate knowledge:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()\n",
    "\n",
    "knowledge"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Answer briefly: Are lichens harmful to trees?\n",
    "\n",
    "    knowledge: $knowledge\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This technique allows the model to first gather relevant knowledge and then use it to form a more accurate and informative response."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt Chaining\n",
    "\n",
    "Prompt Chaining is a technique where a complex task is broken down into several sequential subtasks.\n",
    "The answer from one prompt becomes the input data for the next, creating a chain of operations.\n",
    "This allows solving complex tasks, increases transparency, controllability, and reliability when working with LLMs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val ctryCap = chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Extract all mentions of countries and their capitals from the following text.  The answer should be a list in the format \"Country: Capital\".\n",
    "\n",
    "    Text:\n",
    "    France is famous for the Eiffel Tower in Paris, and Germany is known for its automotive industry with headquarters in Berlin.  Meanwhile, tourists enjoy the picturesque views of Rome in Italy and the castles near Madrid in Spain.\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Based on the following list of countries and their capitals, create a short guide to the three most interesting sights in each capital city:\n",
    "\n",
    "    $ctryCap\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Working with prompts is fundamental for both AI application users and developers.\n",
    "Spring AI and Kotlin provide a convenient and powerful API for this purpose.\n",
    "\n",
    "Check out the next notebook to learn more about Kotlin and Spring AI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
