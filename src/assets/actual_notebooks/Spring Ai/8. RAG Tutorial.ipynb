{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a capability that allows LLMs to access relevant,\n",
    "private data that wasn't available during model training.\n",
    "It also enables more personalized responses.\n",
    "RAG solves the problem of model knowledge becoming outdated,\n",
    "while being more cost-effective than retraining the model.\n",
    "\n",
    "## How RAG Works\n",
    "1. First, you convert your documents, knowledge base, or other content into special vector embeddings (think of them as digital fingerprints of information)\n",
    "2. When a question comes in,\n",
    "   RAG searches these fingerprints to find the most relevant pieces of information\n",
    "3. The system adds this relevant information to the AI's prompt\n",
    "4. The AI creates a response using both its training and this fresh, specific information\n",
    "\n",
    "As in previous notebooks, let's start with some initial setup"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:19:14.569056Z",
     "start_time": "2025-12-01T15:19:12.369170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-openai\n",
    "USE { dependencies { implementation(\"org.springframework.ai:spring-ai-advisors-vector-store:1.1.0\") } }"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:19:16.148725Z",
     "start_time": "2025-12-01T15:19:15.830712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()\n",
    "val openAiOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .temperature(0.7)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As mentioned above, documents need to be converted into vectors.\n",
    "\n",
    "For this, we'll need an `EmbeddingModel`."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:19:17.435597Z",
     "start_time": "2025-12-01T15:19:17.387313Z"
    }
   },
   "cell_type": "code",
   "source": "val embeddingModel = OpenAiEmbeddingModel(openAiApi)",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now have an `EmbeddingModel`,\n",
    "but we need somewhere to store the vector representations of documents.\n",
    "This is what vector stores are designed for.\n",
    "In our example, we'll use a simple in-memory implementation of a vector store."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:19:18.698189Z",
     "start_time": "2025-12-01T15:19:18.599906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.vectorstore.SimpleVectorStore\n",
    "\n",
    "val vectoreStore = SimpleVectorStore.builder(embeddingModel).build()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we just need to add a document to our store.\n",
    "\n",
    "Let's use a Kotlin FAQ"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:19:21.012921Z",
     "start_time": "2025-12-01T15:19:19.814717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.io.File\n",
    "\n",
    "val doc = Document(File(\"data/kotlinFAQ.md\").readText())\n",
    "vectoreStore.add(listOf(doc))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we've prepared everything necessary,\n",
    "let's use the `QuestionAnswerAdvisor`, which implements RAG in Spring-AI.\n",
    "\n",
    "Here's what will happen:\n",
    "1. Send a query\n",
    "2. The query gets vectorized\n",
    "3. The system searches for the closest match to our query vector in the vector store\n",
    "4. The closest results are added to the original query as additional context\n",
    "5. Original query along with this additional context is sent to the LLM\n",
    "6. Receive an answer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:21:14.964547Z",
     "start_time": "2025-12-01T15:21:12.521315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.client.advisor.vectorstore.QuestionAnswerAdvisor\n",
    "\n",
    "ChatClient.create(\n",
    "    OpenAiChatModel.builder()\n",
    "        .openAiApi(openAiApi)\n",
    "        .defaultOptions(openAiOptions)\n",
    "        .build()\n",
    ")\n",
    "    .prompt()\n",
    "    .advisors(QuestionAnswerAdvisor.builder(vectoreStore).build())\n",
    "    .user(\"current version of Kotlin?\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The current version of Kotlin is 2.1.20, which was published on March 20, 2025. You can find more information about it on [GitHub](https://github.com/jetbrains/kotlin)."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
