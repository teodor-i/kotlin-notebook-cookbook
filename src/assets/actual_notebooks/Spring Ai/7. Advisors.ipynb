{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advisors\n",
    "\n",
    "Advisors are an interesting feature in Spring-AI that allows you to flexibly intercept,\n",
    "modify, and enhance AI interactions.\n",
    "\n",
    "With `Advisors`, you can:\n",
    "- Add necessary context to user requests\n",
    "- Filter out harmful or sensitive content in AI requests\n",
    "- Track custom metrics\n",
    "- Ensure consistent output structure\n",
    "- And more\n",
    "\n",
    "Let's add dependencies and create a `ChatModel`"
   ]
  },
  {
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2025-12-01T15:11:12.221472Z",
     "start_time": "2025-12-01T15:11:11.130541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:11:12.584541Z",
     "start_time": "2025-12-01T15:11:12.222821Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()\n",
    "val openAiOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .temperature(0.7)\n",
    "    .build()\n",
    "\n",
    "\n",
    "val chatModel = OpenAiChatModel.builder()\n",
    "    .openAiApi(openAiApi)\n",
    "    .defaultOptions(openAiOptions)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's add the `MessageChatMemory` advisor.\n",
    "As the name suggests, this advisor will implement message history,\n",
    "preserving the conversation context.\n",
    "For this Advisor, we'll need a `ChatMemory` instance\n",
    "where messages will be stored."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:14:21.414663Z",
     "start_time": "2025-12-01T15:14:21.357593Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatMemory = MessageWindowChatMemory.builder().build()\n",
    "\n",
    "val chatClient = ChatClient\n",
    "    .builder(chatModel)\n",
    "    .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build())\n",
    "    .build()\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test how this works"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:14:23.656335Z",
     "start_time": "2025-12-01T15:14:22.578172Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatClient.prompt(\"Hi, tell me a joke\").call().content()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Why did the scarecrow win an award? \n",
       "\n",
       "Because he was outstanding in his field!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:14:25.286378Z",
     "start_time": "2025-12-01T15:14:23.656836Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatClient.prompt(\"What is previous message in our chat history?\").call().content()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The previous message in our chat history was a joke: \"Why did the scarecrow win an award? Because he was outstanding in his field!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, the LLM now has access to our message history.\n",
    "\n",
    "Spring-AI includes several predefined Advisors:\n",
    "- `MessageChatMemoryAdvisor`\n",
    "- `PromptChatMemoryAdvisor`\n",
    "- `QuestionAnswerAdvisor`\n",
    "- `RetrievalAugmentationAdvisor`\n",
    "- `SafeGuardAdvisor`\n",
    "- `SimpleLoggerAdvisor`\n",
    "- `VectorStoreChatMemoryAdvisor`\n",
    "\n",
    "And you can create your own custom Advisor as well.\n",
    "\n",
    "Let's do that now.\n",
    "We'll create an Advisor that logs requests and responses by outputting them to our console.\n",
    "To do this, we'll extend `CallAroundAdvisor` and implement the `aroundCall` method"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:17:46.724533Z",
     "start_time": "2025-12-01T15:17:46.639673Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_10_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.client.advisor.api.CallAdvisor\n",
    "import org.springframework.ai.chat.client.advisor.api.CallAdvisorChain\n",
    "\n",
    "class CustomLogger: CallAdvisor {\n",
    "    override fun getName(): String {\n",
    "        return \"CustomLogger\"\n",
    "    }\n",
    "\n",
    "    override fun getOrder(): Int = 0\n",
    "\n",
    "    override fun adviseCall(chatClientRequest: ChatClientRequest, callAdvisorChain: CallAdvisorChain): ChatClientResponse {\n",
    "        println(\"CustomLogger.Before: ${chatClientRequest}\")\n",
    "        val chatClientResponse = callAdvisorChain.nextCall(chatClientRequest)\n",
    "        println(\"CustomLogger.After: ${chatClientResponse}\")\n",
    "        return chatClientResponse\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see our `CustomAdvisor` in action"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:17:51.740272Z",
     "start_time": "2025-12-01T15:17:48.695125Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_11_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"Generate HelloWorld in Kotlin\")\n",
    "    .advisors(CustomLogger())\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLogger.Before: ChatClientRequest[prompt=Prompt{messages=[UserMessage{content='Hi, tell me a joke', metadata={messageType=USER}, messageType=USER}, AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=Why did the scarecrow win an award? \n",
      "\n",
      "Because he was outstanding in his field!, metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, annotations=[], index=0, id=chatcmpl-ChztWkrp1ETiltxmxtfPP4OvB1ZMf}], UserMessage{content='What is previous message in our chat history?', metadata={messageType=USER}, messageType=USER}, AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=The previous message in our chat history was a joke: \"Why did the scarecrow win an award? Because he was outstanding in his field!\", metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, annotations=[], index=0, id=chatcmpl-ChztXs2RCzayFqZlJQkNppJrLkOOE}], UserMessage{content='Generate HelloWorld in Kotlin', metadata={messageType=USER}, messageType=USER}], modelOptions=OpenAiChatOptions: {\"streamUsage\":false,\"model\":\"gpt-4o-mini\",\"temperature\":0.7}}, context={}]\n",
      "CustomLogger.After: ChatClientResponse[chatResponse=ChatResponse [metadata={ id: chatcmpl-ChzwrBqARkcbawIpn0trhBxOxmmaR, usage: DefaultUsage{promptTokens=90, completionTokens=77, totalTokens=167}, rateLimit: { @type: org.springframework.ai.openai.metadata.OpenAiRateLimit, requestsLimit: 30000, requestsRemaining: 29999, requestsReset: PT0.002S, tokensLimit: 150000000; tokensRemaining: 149999917; tokensReset: PT0S } }, generations=[Generation[assistantMessage=AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=Here’s a simple \"Hello, World!\" program in Kotlin:\n",
      "\n",
      "```kotlin\n",
      "fun main() {\n",
      "    println(\"Hello, World!\")\n",
      "}\n",
      "```\n",
      "\n",
      "You can run this code in any Kotlin environment, such as an IDE like IntelliJ IDEA or an online Kotlin compiler. Just copy and paste the code, and it will print \"Hello, World!\" to the console when executed., metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, annotations=[], index=0, id=chatcmpl-ChzwrBqARkcbawIpn0trhBxOxmmaR}], chatGenerationMetadata=DefaultChatGenerationMetadata[finishReason='STOP', filters=0, metadata=0]]]], context={}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Here’s a simple \"Hello, World!\" program in Kotlin:\n",
       "\n",
       "```kotlin\n",
       "fun main() {\n",
       "    println(\"Hello, World!\")\n",
       "}\n",
       "```\n",
       "\n",
       "You can run this code in any Kotlin environment, such as an IDE like IntelliJ IDEA or an online Kotlin compiler. Just copy and paste the code, and it will print \"Hello, World!\" to the console when executed."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
