{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "\n",
    "LLMs are becoming increasingly integrated into development workflows,\n",
    "creating demand for frameworks that make AI-based application development accessible.\n",
    "In the JVM and Kotlin ecosystem, Spring is exactly such a framework.\n",
    "Yes, that same Spring that has been around for over 20 years!\n",
    "\n",
    "Spring AI is a new addition that simplifies AI integration into your Kotlin code.\n",
    "Spring AI provides:\n",
    "\n",
    "* A unified API for working with various AI models and LLM providers (OpenAI, Anthropic, Ollama, etc.)\n",
    "* Components for prompt processing and context management\n",
    "* Built-in capabilities for vector stores and RAG applications\n",
    "\n",
    "In this series of posts, we'll explore the core features of Spring AI.\n",
    "Following programming tradition,\n",
    "our first post will be a \"Hello, world!\" of sorts:\n",
    "we'll connect to an LLM provider and ask it to tell us a joke.\n",
    "\n",
    "First, let's add the necessary dependencies.\n",
    "> [!NOTE] We'll be working with Claude models from Anthropic,\n",
    "> but we'll include commented code for OpenAI as well.\n",
    "> This will help you understand the general API approach regardless of which model you choose."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:29.569264Z",
     "start_time": "2025-12-01T13:54:28.861181Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter",
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-anthropic\n",
    "//%use spring-ai-openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use the model, we need to provide an API key.\n",
    "\n",
    "You can obtain this API key from\n",
    "[console.anthropic.com](https://console.anthropic.com/settings/keys)\n",
    "for Anthropic models or from\n",
    "[platform.openai.com](https://platform.openai.com/api-keys)\n",
    "for OpenAI models.\n",
    "\n",
    "Then add the generated API key to your environment variables:\n",
    "\n",
    "[MacOS/Linux]\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "export OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "\n",
    "```\n",
    "\n",
    "[Windows]\n",
    "```shell\n",
    "set ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "set OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "```\n",
    "\n",
    "Let's retrieve the API key from environment variables:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:29.649698Z",
     "start_time": "2025-12-01T13:54:29.571727Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"YOUR_ANTHROPIC_API_KEY\"\n",
    "//val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, let's create a low-level API client for our chosen provider.\n",
    "This only requires the API key:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:29.973968Z",
     "start_time": "2025-12-01T13:54:29.651203Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicApi = AnthropicApi.builder().apiKey(apiKey).build()\n",
    "//val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have our client, we can make a simple model call:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:33.043024Z",
     "start_time": "2025-12-01T13:54:29.975696Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicMessage = AnthropicApi.AnthropicMessage(\n",
    "    listOf(AnthropicApi.ContentBlock(\"Tell me a joke\")), AnthropicApi.Role.USER\n",
    ")\n",
    "//val openAiMessage = OpenAiApi.ChatCompletionMessage(\"Tell me a joke\", OpenAiApi.ChatCompletionMessage.Role.USER)\n",
    "\n",
    "\n",
    "anthropicApi.chatCompletionEntity(\n",
    "    AnthropicApi.ChatCompletionRequest(\n",
    "        AnthropicApi.ChatModel.CLAUDE_SONNET_4_0.value,\n",
    "        listOf(anthropicMessage), null, 100, 0.8, false\n",
    "    )\n",
    ")\n",
    "//openAiApi.chatCompletionEntity(\n",
    "//    OpenAiApi.ChatCompletionRequest(listOf(openAiMessage), OpenAiApi.ChatModel.CHATGPT_4_O_LATEST.value, 0.8, false)\n",
    "//)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 OK OK,ChatCompletionResponse[id=msg_01PByqU55f2W72YbZ5WgcZLA, type=message, role=ASSISTANT, content=[ContentBlock[type=TEXT, source=null, text=Why don't scientists trust atoms?\n",
       "\n",
       "Because they make up everything!, index=null, id=null, name=null, input=null, toolUseId=null, content=null, signature=null, thinking=null, data=null, cacheControl=null, title=null, context=null, citations=null]], model=claude-sonnet-4-20250514, stopReason=end_turn, stopSequence=null, usage=Usage[inputTokens=11, outputTokens=17, cacheCreationInputTokens=0, cacheReadInputTokens=0]],[:status:\"200\", anthropic-organization-id:\"ea22ac71-22a7-43cc-9c6a-0a6a2c25768f\", anthropic-ratelimit-input-tokens-limit:\"16500000\", anthropic-ratelimit-input-tokens-remaining:\"16500000\", anthropic-ratelimit-input-tokens-reset:\"2025-12-01T13:54:31Z\", anthropic-ratelimit-output-tokens-limit:\"4250000\", anthropic-ratelimit-output-tokens-remaining:\"4250000\", anthropic-ratelimit-output-tokens-reset:\"2025-12-01T13:54:32Z\", anthropic-ratelimit-tokens-limit:\"20750000\", anthropic-ratelimit-tokens-remaining:\"20750000\", anthropic-ratelimit-tokens-reset:\"2025-12-01T13:54:31Z\", cf-cache-status:\"DYNAMIC\", cf-ray:\"9a7313aafac74f3e-TXL\", content-length:\"473\", content-type:\"application/json\", date:\"Mon, 01 Dec 2025 13:54:33 GMT\", request-id:\"req_011CVg5cfE9hAHNeSPPc3xUi\", retry-after:\"29\", server:\"cloudflare\", strict-transport-security:\"max-age=31536000; includeSubDomains; preload\", x-envoy-upstream-service-time:\"2647\", x-robots-tag:\"none\"]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Congratulations, that's our first joke!\n",
    "\n",
    "As you've noticed, this API doesn't look very convenient,\n",
    "but that's the nature of low-level APIs.\n",
    "\n",
    "Let's raise the abstraction level.\n",
    "We'll create `*ChatOptions` to help us set and use parameters for our LLM provider requests:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:33.080994Z",
     "start_time": "2025-12-01T13:54:33.043942Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicOptions = AnthropicChatOptions.builder()\n",
    "    .model(AnthropicApi.ChatModel.CLAUDE_SONNET_4_0)\n",
    "    .temperature(0.7)\n",
    "    .maxTokens(1024)\n",
    "    .build()\n",
    "//val openAiOptions = OpenAiChatOptions.builder()\n",
    "//    .model(OpenAiApi.ChatModel.CHATGPT_4_O_LATEST)\n",
    "//    .temperature(0.7)\n",
    "//    .build()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's create a `*ChatModel`, which will help us get another joke:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:33.117211Z",
     "start_time": "2025-12-01T13:54:33.081698Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_9_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicChat = AnthropicChatModel.builder()\n",
    "    .anthropicApi(anthropicApi)\n",
    "    .defaultOptions(anthropicOptions)\n",
    "    .build()\n",
    "//val openAiChat = OpenAiChatModel.builder()\n",
    "//    .openAiApi(openAiApi)\n",
    "//    .defaultOptions(openAiOptions)\n",
    "//    .build()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's ask for a joke about Kotlin:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:37.212823Z",
     "start_time": "2025-12-01T13:54:33.117776Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_10_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "anthropicChat.call(\"Tell me a joke about Kotlin\")\n",
    "//openAiChat.call(\"Tell me a joke about Kotlin\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Here's a Kotlin joke for you:\n",
       "\n",
       "Why did the Java developer switch to Kotlin?\n",
       "\n",
       "Because they were tired of writing `NullPointerException` love letters to their code! ðŸ’•\n",
       "\n",
       "(In Kotlin, null safety is built into the type system, so you're much less likely to encounter those pesky NPEs that Java developers know all too well!)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now we have a joke about Kotlin!\n",
    "\n",
    "This API is still specific to a particular provider.\n",
    "You might need an even higher level of abstraction,\n",
    "where your logic doesn't depend on which model you're using.\n",
    "\n",
    "For this, we'll use the `ChatClient`:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T13:54:41.063011Z",
     "start_time": "2025-12-01T13:54:37.227811Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_11_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatClient = ChatClient.create(anthropicChat)\n",
    "chatClient.prompt(\"Tell me a joke about Kotlin\").call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Why did the Java developer switch to Kotlin?\n",
       "\n",
       "Because they were tired of writing `public static void main(String[] args)` when they could just write `fun main()` and actually have fun! \n",
       "\n",
       "ðŸŽ¯ *Bonus punchline: They also got tired of null pointer exceptions ruining their day!*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Congratulations! Now we can write a simple joke and anecdote application.\n",
    "\n",
    "Check out the next notebooks to learn more about Kotlin and Spring AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
